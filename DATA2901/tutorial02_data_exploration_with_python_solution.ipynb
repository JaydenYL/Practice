{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 EXAMPLE SOLUTION: Data Exploration with Python\n",
    "\n",
    "## Data Exploration with Python\n",
    "\n",
    "## Exercise 1. Reading and Accessing a CSV File\n",
    "\n",
    "We start by importing python libraries to read in a .csv file. We then visualise some rows of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas\n",
    "\n",
    "Pandas is a python data processing and analysis library that greatly eases data accessibility and usability to data sources. You can find more information in the [Week 02 lectures, from Page 15, slide 30](https://canvas.sydney.edu.au/courses/21543/files/9665910?module_item_id=759960).\n",
    "\n",
    "Primarily we will be using pandas to load data from sources such as csv into it's DataFrame data-type.\n",
    "A DataFrame is similar to a table in structure - it has rows indexed like an array. Each row has a list of fields. Each of those fields has a corresponding Heading.\n",
    "\n",
    "We will be using the csv output of our Programming Experience Survey from Week 01.\n",
    "\n",
    "Let's first import the relevant libraries that we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import our CSV file named: 'programming_experience_survery_2020.csv'.\n",
    "\n",
    "To do this in pandas we will use the 'read_csv(*filename*)' function and assigning the result to a new **DataFrame** variable.\n",
    "\n",
    "Once we have imported the data, we can see the 'shape' of our data (how many rows and fields) to check whether what we have loaded is correct.\n",
    "\n",
    "We can also use the 'head' operation of our DataFrame variable to see the first few rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "rawData = pd.read_csv('programming_experience_survey_2019.csv')\n",
    "print(rawData.shape)\n",
    "rawData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 TODO for you Visualise the 9th, 15th, 31st, and 2nd last records\n",
    "\n",
    "Display the 9th, 15th, 31th and the 2nd last record of the dictionary (hint: create an array of the appropriate indices annd use the DataFrame 'iloc' function to filter your DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idxLs = np.append(np.array([9,15,31]) - 1, -2)\n",
    "rawData.iloc[idxLs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Rename dataframe columns\n",
    "\n",
    "Usually when we receive data, the column/field labels are very long and descriptive. However, when we want to process data, we usually like to relabel columns/fields to make it easier to type.\n",
    ":\n",
    "First let's have a look at list of the column headings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a working copy of our data so that we don't modify our original data. This a common practice to make sure we do not accidently corrupt our initial input data.\n",
    "\n",
    "Once we have our working copy and our above list of headings, it's a simple matter of using the DataFrame 'rename(*rename_target*=*mapping dictionary of old to new field names*, inplace=True)' function to change the column/field headers within our working copy. The 'inplace=True' means modify the existing items in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a working copy of the raw data\n",
    "wrkData = rawData.copy()\n",
    "\n",
    "# Rename columns\n",
    "wrkData.rename(columns={\n",
    "    'respondent_id': 'ResId',\n",
    "    'submitted': 'SubmitDateTime',\n",
    "    'Tutorial Class': 'ClassId',\n",
    "#     'Degree',\n",
    "    'Programming Experience': 'PrgExp',\n",
    "    'Experience in Python': 'PyExp',\n",
    "    'Used Jupyter Notebooks': 'JupyterExp',\n",
    "    'Other Programming Languages Competency': 'OtherExp',\n",
    "    'SQL Competency': 'SQLExp',\n",
    "    'Relational Databases Competency': 'RelDBExp'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are loading from CSV, we do not have type association for our columns/fields (in fact, everything is considered to be a string). Type information is important because it can alter how the same operations affect the same data but with different types - consider a string vs a number vs a date:\n",
    "\n",
    "   **\"3\" + \"4\" = \"34\"**\n",
    "   \n",
    "   **3 + 4 = 7**\n",
    "   \n",
    "   **3s + 4h = 4:00:03**\n",
    "   \n",
    "Let's try converting the 'SubmitDateTime' column/field to a date type and view our data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "wrkData['SubmitDateTime'] = pd.to_datetime(wrkData['SubmitDateTime'])\n",
    "\n",
    "# Check df column data types\n",
    "print(wrkData.dtypes)\n",
    "\n",
    "# View\n",
    "wrkData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2. Data Cleaning and Conversion\n",
    "\n",
    "It's often the case that form data is collected in terms of text data, but we wish to convert this to a numerical value. A common practice among questions that ascertain some sort of increasing level of respondance is to convert to a Likert scale - this is an escalating series of numbers that replaces each level of respondance.\n",
    "\n",
    "### 2.1 Map Columns values to digits\n",
    "Map the {PythonExp} column values as follows:\n",
    "- 'None' = 1\n",
    "- 'Basic understanding' = 2\n",
    "- 'Written some simple Python programs' = 3\n",
    "- 'Competent Python programmer (familiar with eg. functions and classes)' = 4\n",
    "- 'Have written complex Python programs already.' = 5\n",
    "\n",
    "Let's examine the particular responses we received for 'Experience in Python'.\n",
    "\n",
    "This can be achieved using the DataFrame 'groupby' and 'size' functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get distinct list of {PythonExp} values\n",
    "# wrkData.groupby('PyExp').size().reset_index(name='NumObs') # df\n",
    "wrkData.groupby('PyExp').size() # Series\n",
    "# wrkData.groupby('PyExp').agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO for you - Map column values\n",
    "\n",
    "Next we have to use our mapping established above to introduce a new field/column that contains the Likert scale values for our Python experience. You can label the new column/field 'PyExpLikert'.\n",
    "\n",
    "Hint: Use the DataFrame command 'map(*mapping dictionary of old to new field values*)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO - Add a new field / column called 'PyExpLikert' that changes the Experience in Python responses to a Likert Scale\n",
    "wrkData['PyExpLikert'] = wrkData['PyExp'].map({\n",
    "    'None': 1,\n",
    "    'Basic understanding': 2,\n",
    "    'Written some simple Python programs': 3,\n",
    "    'Competent Python programmer (familiar with eg. functions and classes)': 4,\n",
    "    'Have written complex Python programs already.': 5\n",
    "})\n",
    "wrkData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Number of Other Languages Experience\n",
    "\n",
    "Deﬁne the cardinality for each respondent by considering responses in regards to their experience described in 'Other programming languages competency'. \n",
    "\n",
    "For example, your response mapping should work as follows:\n",
    "\n",
    "'C#, Java, Javascript/ECMAScript, Matlab' = 4\n",
    "\n",
    "'Matlab,R' = 2 \n",
    "\n",
    "'Haskell' = 1\n",
    "\n",
    "... and so on. \n",
    "\n",
    "### TODO by you - Identify the number of other languages {OtherExpCnt} for each respondent. \n",
    "\n",
    "Add a new field/column to your working data ('OtherExpCnt') that will contain the converted cardinality values calculated above. \n",
    "\n",
    "Hint: consider the number of comma separated values in each cell. Try using the 'count(*character*)' property of strings in addition to the DataFrame mapping ability. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - Identify the number of other languages {OtherExpCnt} for each respondent. \n",
    "\n",
    "wrkData['OtherExpCnt'] = wrkData['OtherExp'].str.count(',') + 1\n",
    "wrkData['OtherExpCnt'].apply(lambda x: x+1 if x >= 1 else 0)\n",
    "wrkData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TODO by you - Number of Relational Database Competency {RelDBExp}\n",
    "\n",
    "We also wish to identify the relation database experience as a numerical value as well. In a manner similar to the task above, identify the number of relational database compentencies and store it in a new field/column {RelDBExp} in our working data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - Number of Relational Database Competency {RelDBExp}\n",
    "\n",
    "wrkData['RelDBExpCnt'] = wrkData['RelDBExp'].str.count(',') + 1\n",
    "wrkData['RelDBExpCnt'].apply(lambda x: x+1 if x >= 1 else 0)\n",
    "wrkData.head()\n",
    "# wrkData[['RelDBExp', 'RelDBExpCnt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TODO by you - Encode SQL Competency {SQLExpLikert}\n",
    "\n",
    "Similar to 2.1, encode a new field/column {SQLExpLikert} using the following encoding for the Likert scale of the 'SQLExp' field/column:\n",
    "- 'Never heard of SQL' = 1\n",
    "- 'Heard of it, but never used it' = 2\n",
    "- 'Can interpret some SQL statements' = 3\n",
    "- 'Written some SQL queries already' = 4\n",
    "- 'Can already create tables and write complex SQL queries' = 5\n",
    "\n",
    "First check distinct values of {SQLExp}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wrkData['SQLExp'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - Encode SQL Competency {SQLExpLikert}\n",
    "\n",
    "wrkData['PyExpLikert'] = wrkData['SQLExp'].map({\n",
    "    'Never heard of SQL.': 1,\n",
    "    'Heard of it, but never used it.': 2,\n",
    "    'Can already create tables and write complex SQL queries.': 5,\n",
    "    'Can interpret some SQL statements.': 3,\n",
    "    'Written some SQL queries already.': 4,\n",
    "})\n",
    "wrkData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 (ADV only) TODO by you - Dealing with cleaning many varied strings\n",
    "\n",
    "Define a Python method to clean the 'Tutorial class' column by analysing the texts in cells as follows (it will be written in all kinds of variants, the the following as a general mapping):\n",
    "\n",
    "- SIT Lab 117 (Friday 8am-10am) = F08A\n",
    "- SIT Lab 116 (Friday 8am-10am) = F08B\n",
    "- SIT Lab 118 (Friday 8am-10am) = F08C\n",
    "- SIT Lab 115 (Friday 8am-10am) (online) = F08D\n",
    "- SIT Lab 114 advanced (Friday 8am-10am) = F08ADV\n",
    "- SIT Lab 118 (Friday 10am-12pm) (online) = F10A\n",
    "- SIT Lab 117 (Friday 10am-12pm) = F10B\n",
    "- SIT Lab 116 (Friday 10am-12pm) = F10C\n",
    "- SIT Lab 115 (Friday 10am-12pm) = F10D\n",
    "- SIT Lab 114 advanced (Friday 10am-12pm) = F10ADV\n",
    "- SIT Lab 115 (Friday 12-2pm) = F12A\n",
    "- SIT Lab 114 (Friday 12-2pm) (online) = F12A\n",
    "- SIT Lab 116 (Friday 12-2pm) = F12C\n",
    "- SIT Lab 117 (Friday 12-2pm) = F12D\n",
    "- SIT Lab 115 (Friday 2-4pm) = F14A\n",
    "- SIT Lab 117 (Friday 2-4pm) = F14B\n",
    "- SIT Lab 116 (Friday 2-4pm) = F14C\n",
    "- SIT Lab 114 (Friday 12-2pm) (online) = F14D\n",
    "- SIT Lab 115 (Friday 4-6pm) = F16A\n",
    "\n",
    "All strings need to be cleaned and converted such that this column contains only the given (F08A/B/C/D, F10A/B/C/D, F12A/B/C/D, F14A/B/C/D, F16A) codes. All values which do not fit into this pattern (e.g. a single ”Yes” answer) should be converted to 'NA'.\n",
    "\n",
    "You can begin with the cleaning function provided in the lectures. Note that this function changes data in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(data, column_key, convert_function, default_value, special_values):\n",
    "#     special_values= {} # we provide the special values\n",
    "    for row in data:\n",
    "        old_value = row[column_key]\n",
    "        new_value = default_value\n",
    "        try:\n",
    "            if old_value in special_values.keys():\n",
    "                new_value = special_values[old_value]\n",
    "            else:\n",
    "                new_value = convert_function(old_value)\n",
    "        except (ValueError, TypeError):\n",
    "            print('Replacing {} with {} in column {}'.format(row[column_key], new_value, column_key))\n",
    "        row[column_key] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Analysing Date and Time\n",
    "\n",
    "Usually we need to do very specific operations on dates and times. These will usually involve seperating particular components or calculating durations.\n",
    "\n",
    "### 3.1 TODO by you - Extract the Date {SubmitDate} and Time {'SubmitTime} components of the Submitted column/field\n",
    "\n",
    "Create a new field/column {SubmitDate} for the Date component and a new field/column {SubmitTime} for the Time component from the submission time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - Extract the Date {SubmitDate}\n",
    "# Get date component\n",
    "wrkData['SubmitDate'] = wrkData['SubmitDateTime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - Extract the Time {SubmitDate}\n",
    "# Get time component\n",
    "wrkData['SubmitTime'] = wrkData['SubmitDateTime'].dt.time\n",
    "wrkData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Data Visualisations\n",
    "\n",
    "Data visualisation are an important part of analysing data and gaining insights. \n",
    "\n",
    "In this exercise, you must use a frequency plot and a box plot to visualise the data we have gathered and cleaned.\n",
    "\n",
    "### 4.1 Frequency Plot / Histogram\n",
    "\n",
    "Show a frequency plot of the 'Python Experience' as given by the respondents. In this plot, the x-axis should contain the Python Experience in level (1, 2, 3, 4, 5...) and y-axis should contain how many respondents there were for each level. Bar chart plotting reference: https://pythonspot.com/matplotlib-bar-chart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pyExpFreq = wrkData.groupby('PyExpLikert').size().reset_index(name='NumObs')\n",
    "print(pyExpFreq)\n",
    "\n",
    "# Plot\n",
    "plt.bar(pyExpFreq['PyExpLikert'], pyExpFreq['NumObs'], alpha=0.5, align='center')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Python Experience - Survey 2020')\n",
    "plt.xlabel('Python Experience Level')\n",
    "plt.ylabel('Count')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Boxplots for Likert-Scale\n",
    "\n",
    "Using boxplots this time, visualise boxplots for {OtherExpCnt} and {RelDBExpCnt}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# f,ax = plt.subplots()\n",
    "# ax.boxplot(wrkData['OtherExpCnt'])\n",
    "# ax.set_title('Other Languages Experience')\n",
    "\n",
    "wrkData.boxplot(['OtherExpCnt', 'RelDBExpCnt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5. (Adv only) Integrating Unix into Jupyter Notebook\n",
    "\n",
    "The general idea of the advanced exercise is to solve a few of the data cleaning and analysis steps with the help of some common Unix commands in some of thenotebook cells (cf. advanced seminar this week).\n",
    "\n",
    "### 5.1 TODO by you - Python competency\n",
    "\n",
    "Determine  the  number  of  responses  for  each  Python  programming  competency level using Unix commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - use Unix commands to determine the number of competency for each level of python programming\n",
    "csvtool format '%(6)\\n' programming_experience_survey_2020.csv | sort | uniq -c | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TODO by you - Other languages competency\n",
    "\n",
    "Split the responses for additional language skills into individual language answers, and then determine the number of mentionings of each programming language (using at least in part Unix commands)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - split the additional language skills into individual language answers, \n",
    "#  then determine the mentions of each programming language. Look at the wordcount example for inspiration\n",
    "csvtool format '%(8)\\n' programming_experience_survey_2020.csv | tr ',' '\\n' | sed -r '/^\\s*$/d' | sort | uniq -c | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TODO by you - Visualise the results\n",
    "\n",
    "Visual the results from the previous sub-question in a histogram plot. \n",
    "\n",
    "Which languages are most known?\n",
    "\n",
    "Which languages are least known?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO by you - visualise the results of 5.2, you will have to save the results back into a python variable\n",
    "from functools import reduce\n",
    "# TODO by you - visualise the results of 5.2, you will have to save the results back into a python variable\n",
    "vals = list(wrkData['Other Programming Language Competency'].str.split(','))\n",
    "vals = list(filter(lambda x: type(x) is not float, vals))\n",
    "all_vals = []\n",
    "combined = list(reduce(lambda x, all_vals: all_vals + x, vals))\n",
    "vals, counts = np.unique(combined, return_counts=True)\n",
    "plt.bar(vals, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 TODO by you - Do the same for the databases skills. \n",
    "\n",
    "Which database systems are known/used most by students of this class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO by you - repeat 5.2 and 5.3 for the different types of databases systems or skills that students know\n",
    "from functools import reduce\n",
    "# TODO by you - visualise the results of 5.2, you will have to save the results back into a python variable\n",
    "vals = list(wrkData['Relational Databases Competency'].str.split(','))\n",
    "vals = list(filter(lambda x: type(x) is not float, vals))\n",
    "all_vals = []\n",
    "combined = list(reduce(lambda x, all_vals: all_vals + x, vals))\n",
    "vals, counts = np.unique(combined, return_counts=True)\n",
    "plt.bar(vals, counts)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
