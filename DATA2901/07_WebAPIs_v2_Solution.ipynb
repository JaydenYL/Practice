{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Wk7 Solutions: Using Web APIs with JSON and XML to gather data\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, we will continue looking at methods to gather data from Web-based sources - particularly from Web API's.\n",
    "The common source formats will be JSON or XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A review of Scraping Data from a Website\n",
    "\n",
    "Last week we used Python to scrape data from the convict records website: https://convictrecords.com.au\n",
    "\n",
    "We introduced the following Python libraries:\n",
    "- **Request**         for interacting with websites and web services\n",
    "- **Beautiful Soup**  for webpage parsing\n",
    "\n",
    "For more documentation about functions available in BautifulSoup, see here:\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "We then stored the retrieved data in CSV format and inside a DBMS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# EXERCISE 1: Using Web APIs\n",
    "\n",
    "In this excersize, we are looking at some examples on how to access web APIs which are specifically provided for program to retrieve data. The advantage is that the data is well defined - no distracting HTML tags in between.\n",
    "\n",
    "But the services uses two different formats - either JSON or XML.\n",
    "\n",
    "For **JSON**, we will use the standard language support in Python and its **request** library.<br>\n",
    "For **XML**, we will use the **lxml** parser library.\n",
    "\n",
    "### Example 1: U.S. Government Website Analytic API\n",
    "\n",
    "First some JSON example from the U.S. government website analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of people who visited a U.S. government website using Internet Explorer 6.0 in the last 90 days\n",
    "import requests\n",
    "response = requests.get(\"https://analytics.usa.gov/data/live/ie.json\")\n",
    "print(response.json()['totals']['ie_version']['6.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ouch - Internet Explorer 6.0 which was release in 2001 seems still in use in 2020...\n",
    "\n",
    "Which version of Internet Explorer is used most at this moment when contacting the U.S. government website. For this, we need to look at the actual JSON response. For this it is helpful to have a 'pretty-print' of the corresponding JSON data which is returned by the analytics.use.gov website. The Python **json** library can do this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw response from the U.S. government website\n",
    "import requests\n",
    "response = requests.get(\"https://analytics.usa.gov/data/live/ie.json\")\n",
    "\n",
    "import json\n",
    "print(json.dumps(response.json(), indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above's data, it seems that IE version 11.0 is currently the most popular version of internet explorer used.\n",
    "<br>Amazingly, there are still a few visits with IE 4.0 though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: ABS Population Clock API\n",
    "\n",
    "The Australian Bureau of Statistics provides the following web API os a *Population Clock Web Service* which gives some statistics about the current Australian population. The meaning of the various fields are explained here: http://www.abs.gov.au/AUSSTATS/abs@.nsf/Latestproducts/1420.0.55.001Main%20Features2User%20Guide?opendocument&tabname=Summary&prodno=1420.0.55.001&issue=User%20Guide&num=&view="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "response = requests.get(\"http://www.abs.gov.au/api/demography/populationprojection\")\n",
    "print(json.dumps(response.json(), indent=4, sort_keys=True))\n",
    "\n",
    "population = response.json()['popNow']\n",
    "print(\"Current population in Australia: \"+str(population))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Map APIs\n",
    "Here's another example with parameters send to a web service:\n",
    "\n",
    "There are several MAP API systems that allow you to convert a location address to a GPS location (and some information more). The most popular of these is Google's Maps Platform. However, this underwent an access change in June 2018 which meant that it is now requires an API key and associated billing information.\n",
    "\n",
    "So instead, we will be using the Open Street Maps project at https://www.openstreetmap.org. \n",
    "Note that this service does have however a restriction of 1 API call per second. \n",
    "The following example looks up the GPS location of the School of IT building at \"1 Cleveland Street, Darlington, Australia\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup of a given address via Open Street Maps Wep-API:\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "def waitrequest(base_url, my_params):\n",
    "    # wait 5 second before we make the request, ideally we can prevent too many requests this way \n",
    "    # if we do too many requests, the whole uni's IP range can be locked out!\n",
    "    classsize = 25\n",
    "    sleeptime = random.randint(1,classsize)\n",
    "    print(\"waiting for \"+str(sleeptime)+\" seconds based on a class size of \"+str(classsize))\n",
    "    time.sleep(sleeptime)\n",
    "    return requests.get(base_url, params = my_params)\n",
    "\n",
    "base_url = 'https://nominatim.openstreetmap.org/search'\n",
    "my_params= {'q': '1 Cleveland Street,Darlington,Australia','format':'json','polygon' : 1, 'addressdetails': 1 }\n",
    "\n",
    "response = waitrequest(base_url, my_params)\n",
    "results  = response.json()\n",
    "if (results):\n",
    "    # Check what the results look like\n",
    "    print(\"This is what the response looks like:\")\n",
    "    pprint.pprint(results)\n",
    "    if (len(results) > 0):\n",
    "        x_geo    = results[0]\n",
    "        print(\"Here is the Longitude and Latitude of our school:\")\n",
    "        print(x_geo['lon'], x_geo['lat'])\n",
    "else:\n",
    "    print(\"no results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR TASK: Retrieve Geo-Location of Arrival Ports of Convict Ship 'Adelaide' (SOLUTION)\n",
    " - Where lies 'Van Diemen's Land'?<br>\n",
    "   Use the Open Street Maps Web-API to check for the *GPS location* of the landing locations of the first voyage of the convict transportation ship \"Adelaide\" (cf. Exercise 1b): **Port Phillip** and **Van Diemen's Land**\n",
    " - Also retrieve the 'boundingbox'.<br>\n",
    "   For this you might need to inspect first how the JSON response is structured: Do hence first a pretty-print of the corresponding JSON response data.\n",
    " - Tip: if you want to see a map for a given GPS location, try: https://www.latlong.net/\n",
    " - Discuss: How would you store this information in your relational database next to the passenger list information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the content of this cell with your Python solution\n",
    "## You might have to use the more generic API for your request\n",
    "## my_q1_params= {'q': 'Port Phillip','format':'json'}\n",
    "## my_q2_params= {'q': 'Van Diemen\\'s Land','format':'json'}\n",
    "\n",
    "## remember that you should always have a wait time so use the waitrequest() function to make a request.\n",
    "\n",
    "base_url = 'https://nominatim.openstreetmap.org/search'\n",
    "my_q1_params= {'q': 'Port Phillip','format':'json'}\n",
    "my_q2_params= {'q': 'Van Diemen\\'s Land','format':'json'}\n",
    "\n",
    "response = waitrequest(base_url, my_q1_params)\n",
    "results_q1  = response.json()\n",
    "if (results_q1):\n",
    "    # Check what the results look like\n",
    "    print(\"This is what the response looks like:\")\n",
    "    pprint.pprint(results_q1)\n",
    "    if (len(results_q1) > 0):\n",
    "       x_geo    = results_q1[0]\n",
    "       print(\"Here is the Longitude and Latitude of Port Philip:\")\n",
    "       print(x_geo['lon'], x_geo['lat'])\n",
    "else:\n",
    "    print(\"no results\")\n",
    "    \n",
    "response = waitrequest(base_url, my_q2_params)\n",
    "results_q2  = response.json()\n",
    "if (results_q2):\n",
    "    # Check what the results look like\n",
    "    print(\"This is what the response looks like:\")\n",
    "    pprint.pprint(results_q2)\n",
    "    if (len(results_q2) > 0):\n",
    "       x_geo    = results_q2[0]\n",
    "       print(\"Here is the Longitude and Latitude of Van Diemen's Land:\")\n",
    "       print(x_geo['lon'], x_geo['lat'])\n",
    "       print(\"Here is what the bounding box looks like:\")\n",
    "       print(x_geo['boundingbox'])\n",
    "else:\n",
    "    print(\"no results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Web API returning XML\n",
    "\n",
    "Some web APIs return data in **XML** format.\n",
    "The easiest library to work with such kind of data in Python is called **lxml**.\n",
    "Its documentation can be found here:<br>\n",
    "http://lxml.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### In the \"Justice News\" RSS feed maintained by the Justice Department, the number of items published on a Friday\n",
    "from datetime import datetime\n",
    "from lxml import etree\n",
    "import requests\n",
    "url = 'https://www.justice.gov/feeds/opa/justice-news.xml'\n",
    "news= requests.get(url).content\n",
    "doc = etree.fromstring(requests.get(url).content)\n",
    "items = doc.xpath('//channel/item')\n",
    "\n",
    "# how many news items on last Friday?\n",
    "dates = [item.find('pubDate').text.strip() for item in items]\n",
    "ts = [datetime.strptime(d[0:16], \"%a, %d %b %Y\") for d in dates]\n",
    "# for weekday(), 4 correspond to Friday\n",
    "print(len([t for t in ts if t.weekday() == 4]))\n",
    "\n",
    "# which news items were this\n",
    "titles = [item.find('title').text for item in items]\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: GitHub API\n",
    "Some final more complex example, extracting some information from the meta-data of a GitHub repository.\n",
    "\n",
    "This is also an example of how to use the Pandas library to work with JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the lecture slides: list of programming languages used in PostgreSQL for the last 5 updated repositories according to GitHub repositories\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://api.github.com/users/postgres/repos'\n",
    "response = requests.get(base_url)\n",
    "\n",
    "df = pd.read_json(response.text)\n",
    "\n",
    "df.sort_values(by='updated_at', ascending=False, inplace=True)\n",
    "df[['name','language','updated_at']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Web API References\n",
    "\n",
    "If you are further interested in exploring some web APIs, have a look at the following lists:\n",
    "\n",
    "101 Data Journalist Challenges\n",
    "https://github.com/stanfordjournalism/search-script-scrape\n",
    "\n",
    "Tutorial on how to use New York Times API (needs registration with NYT)\n",
    "https://stanford.edu/~vbauer/teaching/nyt.html\n",
    "\n",
    "NSW Public Transport Events (needs registration)\n",
    "https://opendata.transport.nsw.gov.au/dataset/public-transport-realtime-alerts-0\n",
    "\n",
    "Twitter web API (needs registration):\n",
    "https://developer.twitter.com/en/docs/basics/getting-started\n",
    "\n",
    "Canvas Web API (needs an API key for cURL access): \n",
    "https://canvas.instructure.com/doc/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In Week 8 (after the break) you will be issued the group assignment for this course. There are some administrative tasks for the assignment that we can get done in last week's tutorial which we will follow up now, as well as some hints that can be provided regarding the content of the assignment\n",
    "\n",
    "## Exercise 1: Forming Groups\n",
    "\n",
    "Form an assignment group with at least one and up to two other students who are commonly in your tutorial class.\n",
    "Then register your group in Canvas under DATA2001 -> People -> Assignment Groups.\n",
    "Make sure your register in the appropriate group number for your tutorial:\n",
    "\n",
    "## Assignment Hints\n",
    "\n",
    "The are three primary components to the assignment:\n",
    "1. Data integration from multiple sources\n",
    "1. Using the integrated data to generate a model / metric\n",
    "1. Writing a report based on some analysis using your model / metric\n",
    "\n",
    "The data integration component is likely to involve the following tasks:\n",
    "1. As a starting point, we will provide you with a few initial data sets (in CSV format)\n",
    "    2. You will have to load those datasets into your postgresql database\n",
    "1. We will also provide some links to some GeoData (which will be covered in week 9)\n",
    "1. You will have to load this GeoData into your postgresql database as well (the university server accounts you have been given should handle GIS data)\n",
    "1. We will then give you a model or a metric to calculate based on the data you have been provided\n",
    "1. You will have to augment the data for the model by gathering extra data from other sources using the Web scraping techniques from last week and the WebAPI access techniques from today's tutorial.\n",
    "\n",
    "\n",
    "\n",
    "Once you have a working model/metric, we will ask that you prepare a report based on analysing some statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Books:\n",
    "- Seppe van den Broucke and Bart Baesens: \"Practical Web Scraping for Data Science\", Springer 2018. (available electroinically via USYD library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Tutorial. Many Thanks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
